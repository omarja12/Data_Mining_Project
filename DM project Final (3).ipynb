{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: pyclustertend in c:\\users\\user\\anaconda3\\lib\\site-packages (1.4.9)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.4; however, version 20.3.3 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\user\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install pyclustertend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bioinfokit in c:\\users\\user\\anaconda3\\lib\\site-packages (1.0.4)\n",
      "Requirement already satisfied: tabulate in c:\\users\\user\\anaconda3\\lib\\site-packages (from bioinfokit) (0.8.7)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\anaconda3\\lib\\site-packages (from bioinfokit) (0.23.2)\n",
      "Requirement already satisfied: adjustText in c:\\users\\user\\anaconda3\\lib\\site-packages (from bioinfokit) (0.7.3)\n",
      "Requirement already satisfied: seaborn in c:\\users\\user\\anaconda3\\lib\\site-packages (from bioinfokit) (0.11.0)\n",
      "Requirement already satisfied: statsmodels in c:\\users\\user\\anaconda3\\lib\\site-packages (from bioinfokit) (0.12.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\lib\\site-packages (from bioinfokit) (1.19.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\user\\anaconda3\\lib\\site-packages (from bioinfokit) (3.3.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\user\\anaconda3\\lib\\site-packages (from bioinfokit) (1.5.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\anaconda3\\lib\\site-packages (from bioinfokit) (1.1.3)\n",
      "Requirement already satisfied: matplotlib-venn in c:\\users\\user\\anaconda3\\lib\\site-packages (from bioinfokit) (0.11.6)\n",
      "Requirement already satisfied: textwrap3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from bioinfokit) (0.9.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn->bioinfokit) (0.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn->bioinfokit) (2.1.0)\n",
      "Requirement already satisfied: patsy>=0.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from statsmodels->bioinfokit) (0.5.1)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib->bioinfokit) (2020.6.20)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib->bioinfokit) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib->bioinfokit) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib->bioinfokit) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib->bioinfokit) (1.3.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib->bioinfokit) (8.0.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas->bioinfokit) (2020.1)\n",
      "Requirement already satisfied: six in c:\\users\\user\\anaconda3\\lib\\site-packages (from patsy>=0.5->statsmodels->bioinfokit) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.4; however, version 20.3.3 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\user\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install bioinfokit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded backend module://ipykernel.pylab.backend_inline version unknown.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import datetime as dt\n",
    "import re\n",
    "from os.path import join\n",
    "import os\n",
    "from sklearn.base import clone\n",
    "\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sompy.visualization.mapview import View2D\n",
    "from sompy.visualization.bmuhits import BmuHitsView\n",
    "from sompy.visualization.hitmap import HitMapView\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "from pyclustertend import hopkins\n",
    "from pyclustertend import vat\n",
    "from pyclustertend import ivat\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.cluster import DBSCAN, KMeans, AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MeanShift\n",
    "import sompy\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina' # optionally, you can change 'svg' to 'retina'\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import PVA Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'donors.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-ce0a253cddb9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpva\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'donors.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlow_memory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'CONTROLN'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mpva\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Unnamed: 0'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mpva\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minplace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 946\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1176\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1179\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2007\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2008\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'donors.csv'"
     ]
    }
   ],
   "source": [
    "pva = pd.read_csv('donors.csv', low_memory = False, index_col = 'CONTROLN')\n",
    "pva.drop(columns = 'Unnamed: 0', inplace = True)\n",
    "pva.sort_index(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#give meaninful names to Gender values and group missing values as unknown \n",
    "pva['GENDER'].replace({' ':'U', 'C':'U', 'A':'U'}, regex = True, inplace = True)\n",
    "pva['GENDER'].replace({'F':'Female', 'M':'Male', 'J':'Join', 'U':'Unknown'}, regex = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#extract Urbancy level from Domain feature \n",
    "pva['URBANICITY_LEVEL'] = pva['DOMAIN'].str.split('([0-9])', 1, expand = True).iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#give meaninful names to Urbanicity Level values and group missing values as nan\n",
    "pva['URBANICITY_LEVEL'].replace({' ': np.nan,'U':'Urban', 'C':'City', 'S':'Suburban', 'T':'Town', 'R':'Rural'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename RFA_2F in a meaninful name\n",
    "pva.rename(columns={'RFA_2F': 'GIFT_FREQUENCY'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function that convert all dates to datetime format\n",
    "def date_time(data, column):\n",
    "    '''Transform to date time'''\n",
    "    data[column] = pd.to_datetime(data[column])\n",
    "\n",
    "#create a list containing all the date features\n",
    "date_features = ['ODATEDW', 'DOB', 'ADATE_2', 'ADATE_3', 'ADATE_4', 'ADATE_5', 'ADATE_6', 'ADATE_7', 'ADATE_8', 'ADATE_9', 'ADATE_10', 'ADATE_11', 'ADATE_12', 'ADATE_13', 'ADATE_14', \n",
    "         'ADATE_15', 'ADATE_16', 'ADATE_17', 'ADATE_18', 'ADATE_19', 'ADATE_20', 'ADATE_21', 'ADATE_22', 'ADATE_23', 'ADATE_24', 'MAXADATE', 'RDATE_3', 'RDATE_4', 'RDATE_5',\n",
    "         'RDATE_6', 'RDATE_7', 'RDATE_8', 'RDATE_9', 'RDATE_10', 'RDATE_11', 'RDATE_12', 'RDATE_13', 'RDATE_14',  'RDATE_15', 'RDATE_16', 'RDATE_17', 'RDATE_18', 'RDATE_19',\n",
    "         'RDATE_20', 'RDATE_21', 'RDATE_22', 'RDATE_23', 'RDATE_24', 'MINRDATE', 'MAXRDATE', 'LASTDATE', 'FISTDATE', 'NEXTDATE']\n",
    "\n",
    "#apply the function\n",
    "date_features_list = []\n",
    "for date in date_features:\n",
    "    if date in pva.columns:\n",
    "        date_time(pva, date)\n",
    "        date_features_list.append(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert DOB to Age feature and drop DOB from dataset\n",
    "pva['AGE'] = (pd.Timestamp('today') - pva['DOB']).astype('<m8[Y]')\n",
    "pva.drop('DOB', axis =1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert ODATEDW (date of donors first gift) to number of years since first gift \n",
    "pva['YEARS_FIRST_GIFT'] = (pd.Timestamp('today') - pva['ODATEDW']).astype('<m8[Y]')\n",
    "pva.drop('ODATEDW', axis =1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace missing values of interest features with No\n",
    "donor_interests = ['MBCRAFT', 'MBGARDEN', 'MBBOOKS', 'MBCOLECT', 'MAGFAML', 'MAGFEM', 'MAGMALE','PUBGARDN',\n",
    "                   'PUBCULIN','PUBHLTH','PUBDOITY', 'PUBNEWFN', 'PUBPHOTO','PUBOPP','COLLECT1','VETERANS',\n",
    "                   'BIBLE','CATLG','HOMEE','PETS','CDPLAY','STEREO','PCOWNERS', 'PHOTO','CRAFTS','FISHER',\n",
    "                   'GARDENIN','BOATS','WALKER','KIDSTUFF','CARDS','PLATES']\n",
    "\n",
    "for interest in donor_interests:\n",
    "    if interest in pva.columns:\n",
    "        pva[interest] = pva[interest].replace(' ', 'N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create log variable for continous variables that doesn't have 0\n",
    "pva['LOG_RAMNTALL'] = np.log(pva['RAMNTALL']) \n",
    "pva['LOG_AGE'] = np.log(pva['AGE']) \n",
    "pva['LOG_AVGGIFT'] = np.log(pva['AVGGIFT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a daba base with only the metric features selected\n",
    "pva_metric = pva[['INCOME', 'LASTGIFT', 'MINRAMNT', 'LOG_RAMNTALL','LOG_AGE', 'LOG_AVGGIFT', \n",
    "             'YEARS_FIRST_GIFT', 'NGIFTALL', 'GIFT_FREQUENCY', 'TIMELAG', 'HIT', 'CARDGIFT']]\n",
    "\n",
    "#create a daba base with only the categorical features selected\n",
    "pva_non_metric = pva[['STATE', 'GENDER', 'URBANICITY_LEVEL','COLLECT1','VETERANS','BIBLE','CATLG','HOMEE','PETS',\n",
    "                       'CDPLAY','STEREO','PCOWNERS', 'PHOTO','CRAFTS','FISHER','GARDENIN','BOATS','WALKER',\n",
    "                       'KIDSTUFF','CARDS','PLATES']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list with the names of the metric features\n",
    "pva_metric_list = pva_metric.columns.tolist()\n",
    "\n",
    "#create a list with the names of the categorical features\n",
    "pva_non_metric_list = pva_non_metric.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pva_metric.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pva_metric.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pva_metric.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pva_metric.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-Metric Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pva_non_metric.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pva_non_metric.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pva_non_metric.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a grid to visualize Age, Years_first_gift, Gift_frecuency, Cardgift, Income, Gender and Urbanicity level\n",
    "f = plt.figure(figsize = (30,20))\n",
    "gs = f.add_gridspec(4, 2)\n",
    "\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    \n",
    "    ax = f.add_subplot(gs[0, 0])\n",
    "    sns.histplot(x = 'AGE', data = pva, bins = 10, ax = ax, kde = True, hue = 'INCOME', palette = 'viridis')\n",
    "\n",
    "    ax = f.add_subplot(gs[0, 1])\n",
    "    sns.countplot(x = 'YEARS_FIRST_GIFT', data = pva, ax = ax, palette = 'viridis')\n",
    "    \n",
    "    ax = f.add_subplot(gs[1, 0])\n",
    "    sns.countplot(x = 'GIFT_FREQUENCY', data = pva, ax = ax, palette = 'viridis')\n",
    "\n",
    "    ax = f.add_subplot(gs[1, 1])\n",
    "    sns.countplot(x = 'CARDGIFT', data = pva, ax = ax, palette = 'viridis')\n",
    "    \n",
    "    ax = f.add_subplot(gs[2, 0])\n",
    "    sns.countplot(x = 'INCOME', data = pva, ax = ax, palette = 'viridis')  \n",
    "    \n",
    "    ax = f.add_subplot(gs[2, 1])\n",
    "    sns.countplot(x = 'GENDER', data = pva, ax = ax, palette = 'viridis') \n",
    "    \n",
    "    ax = f.add_subplot(gs[3, 0])\n",
    "    sns.countplot(x = 'URBANICITY_LEVEL', data = pva, ax = ax, palette = 'viridis') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create a grid to plot Minramnt and Lastgift\n",
    "f = plt.figure(figsize = (20,10))\n",
    "gs = f.add_gridspec(2, 1)\n",
    "\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    \n",
    "    ax = f.add_subplot(gs[0, 0])\n",
    "    sns.histplot(x = 'MINRAMNT', bins = 90, data = pva, ax = ax, palette = 'viridis')\n",
    "    plt.xlim([0, 60])\n",
    "        \n",
    "    ax = f.add_subplot(gs[1, 0])\n",
    "    sns.histplot(x = 'LASTGIFT', bins = 90, data = pva, ax = ax, palette = 'viridis') \n",
    "    plt.xlim([0, 60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#visualize log_ramntall\n",
    "a4_dims = (20, 10)\n",
    "fig, ax = pyplot.subplots(figsize = a4_dims)\n",
    "\n",
    "sns.histplot(x = np.log(pva['RAMNTALL']), bins = 30,  ax = ax, palette = 'viridis', kde=True) ## data = pva,\n",
    "plt.xlim([0, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#visualize Log_avggift\n",
    "a4_dims = (20, 10)\n",
    "fig, ax = pyplot.subplots(figsize = a4_dims)\n",
    "\n",
    "sns.histplot(x = np.log(pva['AVGGIFT']), bins = 30,  ax = ax, palette = 'viridis', kde=True) ## data = pva,\n",
    "plt.xlim([0, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#visualize Log_age\n",
    "a4_dims = (20, 10)\n",
    "fig, ax = pyplot.subplots(figsize = a4_dims)\n",
    "\n",
    "sns.histplot(x = np.log(pva['AGE']),  ax = ax, palette = 'viridis', kde=True) ## data = pva,\n",
    "plt.xlim([0, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#visualize Nigiftall\n",
    "sns.displot(x = 'NGIFTALL', data = pva, binwidth= 10, palette='viridis')\n",
    "plt.xlim([0, 50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#visualize Hit\n",
    "sns.displot(x = 'HIT', data = pva, palette = 'viridis', binwidth= 10)\n",
    "plt.xlim([0, 40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualice donnor by state ordered in descending manner\n",
    "a4_dims = (20, 10)\n",
    "fig, ax = pyplot.subplots(figsize = a4_dims)\n",
    "sns.countplot(x = 'STATE', data = pva, ax = ax, palette = 'viridis', \n",
    "                 order = pva['STATE'].value_counts().index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize nterests\n",
    "f = plt.figure(figsize = (20,30))\n",
    "gs = f.add_gridspec(6, 3)\n",
    "\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    \n",
    "    ax = f.add_subplot(gs[0, 0])\n",
    "    sns.countplot(x = 'PLATES', data = pva, ax = ax, palette = 'viridis')\n",
    "    \n",
    "    ax = f.add_subplot(gs[0, 1])\n",
    "    sns.countplot(x = 'COLLECT1', data = pva, ax = ax, palette = 'viridis')\n",
    "    \n",
    "    ax = f.add_subplot(gs[0, 2])\n",
    "    sns.countplot(x = 'VETERANS', data = pva, ax = ax, palette = 'viridis')\n",
    "    \n",
    "    ax = f.add_subplot(gs[1, 0])\n",
    "    sns.countplot(x = 'BIBLE', data = pva, ax = ax, palette = 'viridis')\n",
    "    \n",
    "    ax = f.add_subplot(gs[1, 1])\n",
    "    sns.countplot(x = 'CATLG', data = pva, ax = ax, palette = 'viridis')\n",
    "    \n",
    "    ax = f.add_subplot(gs[1, 2])\n",
    "    sns.countplot(x = 'HOMEE', data = pva, ax = ax, palette = 'viridis')\n",
    "    \n",
    "    ax = f.add_subplot(gs[2, 0])\n",
    "    sns.countplot(x = 'PETS', data = pva, ax = ax, palette = 'viridis')\n",
    "    \n",
    "    ax = f.add_subplot(gs[2, 1])\n",
    "    sns.countplot(x = 'CDPLAY', data = pva, ax = ax, palette = 'viridis')\n",
    "    \n",
    "    ax = f.add_subplot(gs[2, 2])\n",
    "    sns.countplot(x = 'STEREO', data = pva, ax = ax, palette = 'viridis')\n",
    "    \n",
    "    ax = f.add_subplot(gs[3, 0])\n",
    "    sns.countplot(x = 'PCOWNERS', data = pva, ax = ax, palette = 'viridis')\n",
    "    \n",
    "    ax = f.add_subplot(gs[3, 1])\n",
    "    sns.countplot(x = 'PHOTO', data = pva, ax = ax, palette = 'viridis')\n",
    "    \n",
    "    ax = f.add_subplot(gs[3, 2])\n",
    "    sns.countplot(x = 'CRAFTS', data = pva, ax = ax, palette = 'viridis')\n",
    "    \n",
    "    ax = f.add_subplot(gs[4, 0])\n",
    "    sns.countplot(x = 'FISHER', data = pva, ax = ax, palette = 'viridis')\n",
    "    \n",
    "    ax = f.add_subplot(gs[4, 1])\n",
    "    sns.countplot(x = 'GARDENIN', data = pva, ax = ax, palette = 'viridis')\n",
    "    \n",
    "    ax = f.add_subplot(gs[4, 2])\n",
    "    sns.countplot(x = 'BOATS', data = pva, ax = ax, palette = 'viridis')\n",
    "    \n",
    "    ax = f.add_subplot(gs[5, 0])\n",
    "    sns.countplot(x = 'WALKER', data = pva, ax = ax, palette = 'viridis')\n",
    "    \n",
    "    ax = f.add_subplot(gs[5, 1])\n",
    "    sns.countplot(x = 'KIDSTUFF', data = pva, ax = ax, palette = 'viridis')\n",
    "    \n",
    "    ax = f.add_subplot(gs[5, 2])\n",
    "    sns.countplot(x = 'CARDS', data = pva, ax = ax, palette = 'viridis')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Use KNNImputer to fill missing values on numeric features\n",
    "imputer = KNNImputer(n_neighbors=5, weights = 'uniform')\n",
    "pva_metric[['LOG_AGE', 'INCOME']] = imputer.fit_transform(pva_metric[['LOG_AGE', 'INCOME']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list with categorical features with missing values\n",
    "mode_as_missing_value = ['URBANICITY_LEVEL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#fill missing values in non metric features with mode\n",
    "for missing_feature in mode_as_missing_value:\n",
    "    pva_non_metric[missing_feature].fillna(pva_non_metric[missing_feature].mode()[0], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill nan values of Timelag with 0\n",
    "pva_metric['TIMELAG'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check is missing values of categorical features were correctly filled\n",
    "pva_non_metric.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#check is missing values of metric features were correctly filled\n",
    "pva_metric.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an IQR with the 10th and 90th quantiles\n",
    "q10 = pva_metric.quantile(.10)\n",
    "q90 = pva_metric.quantile(.90)\n",
    "iqr = (q90 - q10)\n",
    "\n",
    "upper_lim = q90 + 1.5*iqr\n",
    "lower_lim = q10 - 1.5*iqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a filter with the observations out of the IQR\n",
    "filters = []\n",
    "for metric in pva_metric_list:\n",
    "    llim = lower_lim[metric]\n",
    "    ulim = upper_lim[metric]\n",
    "    filters.append(pva_metric[metric].between(llim, ulim, inclusive=True))\n",
    "\n",
    "filters = pd.Series(np.all(filters, 0), index = pva_metric.index.tolist())   \n",
    "\n",
    "#print the percentage of data kept after applying the filter created above\n",
    "print('Percentage of data kept after removing outliers:', np.round(pva_metric[filters].shape[0]/pva_metric.shape[0], 2))\n",
    "\n",
    "#replace the metric data base for the filtered one\n",
    "pva_metric = pva_metric[filters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace the categorical data base for the filtered one\n",
    "pva_non_metric = pva_non_metric[filters]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a copy of data base with missing values filled and outlier removed\n",
    "pva_standard = pva_metric.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling data using standarscaler to get a mean of 0 and variance of 1\n",
    "scaler = StandardScaler()\n",
    "scaled_feat = scaler.fit_transform(pva_standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a data base with standarized metric features\n",
    "pva_standard[pva_metric_list] = scaled_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pva_standard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 8))\n",
    "\n",
    "#obtain correlation matrix. Round the values to 2 decimal cases.\n",
    "corr_matrix = np.round(pva_metric[pva_metric_list].corr(method='pearson'), decimals=2)\n",
    "\n",
    "#build annotation matrix (values above |0.5| will appear annotated in the plot)\n",
    "mask_annot = np.absolute(corr_matrix.values) >= 0.5\n",
    "annot = np.where(mask_annot, corr_matrix.values, np.full(corr_matrix.shape,''))\n",
    "\n",
    "#plot heatmap of the correlation matrix\n",
    "sns.heatmap(data=corr_matrix, annot=annot, cmap=sns.diverging_palette(220, 10, as_cmap=True), \n",
    "            fmt='s', vmin=-1, vmax=1, center=0, square=True, linewidths=.5)\n",
    "\n",
    "fig.subplots_adjust(top=0.95)\n",
    "fig.suptitle(\"Correlation Matrix\", fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing Clustering Tendency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Assesment Tendency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#apply vat algorithm to dataset \n",
    "vat(pva_standard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hopkins statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#apply Hopkins Statistic to data set\n",
    "hopkins(pva_standard, len(pva_standard))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a copy to encode categorial features\n",
    "pva_non_metric_ohc = pva_non_metric.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply the method OneHotEncoding\n",
    "ohc = OneHotEncoder(sparse=False, drop= 'if_binary')\n",
    "ohc_feat = ohc.fit_transform(pva_non_metric_ohc)\n",
    "ohc_feat_names = ohc.get_feature_names(pva_non_metric_list)\n",
    "ohc_pva = pd.DataFrame(ohc_feat, index = pva_non_metric_ohc.index, columns = ohc_feat_names)  \n",
    "ohc_pva"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a copy to do PCA\n",
    "pva_pca = pva_standard.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply PCA to all metric features \n",
    "pca = PCA(n_components = pva_pca.shape[1], whiten = True)\n",
    "pca_reduced = pca.fit_transform(pva_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following table show the cumulative variance explained by the PC\n",
    "pd.DataFrame(\n",
    "    {\"Eigenvalue\": pca.explained_variance_,\n",
    "     \"Difference\": np.insert(np.diff(pca.explained_variance_), 0, 0),\n",
    "     \"Proportion\": pca.explained_variance_ratio_,\n",
    "     \"Cumulative\": np.cumsum(pca.explained_variance_ratio_)},\n",
    "    index=range(1, pca.n_components_ + 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the number of components have a acumulated variance bigger that 90%\n",
    "np.argmax(np.cumsum(pca.explained_variance_ratio_) >= 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# draw plots\n",
    "ax1.plot(pca.explained_variance_, marker=\".\", markersize=12)\n",
    "ax2.plot(pca.explained_variance_ratio_, marker=\".\", markersize=12, label=\"Proportion\")\n",
    "ax2.plot(np.cumsum(pca.explained_variance_ratio_), marker=\".\", markersize=12, linestyle=\"--\", label=\"Cumulative\")\n",
    "\n",
    "# customizations\n",
    "ax2.legend()\n",
    "ax1.set_title(\"Scree Plot\", fontsize=14)\n",
    "ax2.set_title(\"Variance Explained\", fontsize=14)\n",
    "ax1.set_ylabel(\"Eigenvalue\")\n",
    "ax2.set_ylabel(\"Proportion\")\n",
    "ax1.set_xlabel(\"Components\")\n",
    "ax2.set_xlabel(\"Components\")\n",
    "ax1.set_xticks(range(0, pca.n_components_, 2))\n",
    "ax1.set_xticklabels(range(1, pca.n_components_ + 1, 2))\n",
    "ax2.set_xticks(range(0, pca.n_components_, 2))\n",
    "ax2.set_xticklabels(range(1, pca.n_components_ + 1, 2))\n",
    "plt.xticks(rotation = 90)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list with the PC that explain 90% of the variance\n",
    "pca_feat_names = [f\"PC{i}\" for i in range(np.argmax(np.cumsum(pca.explained_variance_ratio_) >= 0.9))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe with the PC that explain 90% of the variance\n",
    "pca_90_pct = pd.DataFrame(pca_reduced[:,0:6], index= pva_pca.index, columns = pca_feat_names) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot data with the PC\n",
    "plt.scatter(pca_90_pct['PC0'], pca_90_pct['PC1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Number of Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elbow Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_clusters = range(2,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate over a desired ncluster range and save the inertia of the given cluster solution\n",
    "inertia = []\n",
    "for k in range_clusters: \n",
    "    kmclust = KMeans(n_clusters = k, init = 'k-means++', n_init = 15, random_state = 42)\n",
    "    kmclust.fit(pva_standard)\n",
    "    inertia.append(kmclust.inertia_)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot inertia vs number of clusters\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.plot(range_clusters , inertia) # range_clusters ,\n",
    "plt.ylabel(\"Inertia: SSw\")\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.title(\"Inertia plot over clusters\", size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calinski-Harabasz Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Calinski_Harabasz = []\n",
    "for k in range_clusters:\n",
    "    kmeans_model = KMeans(n_clusters = k, init = 'k-means++', n_init = 15,random_state = 42).fit(pva_standard)\n",
    "    labels = kmeans_model.labels_\n",
    "    labels = kmeans_model.labels_ \n",
    "    Calinski_Harabasz.append(metrics.calinski_harabasz_score(pva_standard, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Calinski_Harabasz plot\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.plot(range_clusters , Calinski_Harabasz) \n",
    "plt.ylabel(\"Calinski Harabasz\")\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.title(\"Calinski Harabasz plot over clusters\", size=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "km_clust = KMeans(n_clusters = 4, init = 'k-means++', n_init = 15, random_state = 42)\n",
    "km_labels = km_clust.fit_predict(pva_standard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Characterizing K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pva_standard_km_labeled = pd.concat((pva_standard, pd.Series(km_labels, name='labels', index=pva_standard.index.tolist())), axis=1)\n",
    "km_standard = pva_standard_km_labeled.groupby('labels').mean()\n",
    "km_standard['label'] = km_standard.index\n",
    "km_standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "pd.plotting.parallel_coordinates(km_standard, class_column = 'label', color=sns.color_palette())\n",
    "plt.xticks(fontsize=10, rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pva_metric_km_labeled = pd.concat((pva_metric, pd.Series(km_labels, name='labels', index=pva_standard.index.tolist())), axis=1)\n",
    "km_metric = pva_metric_km_labeled.groupby('labels').mean()\n",
    "km_metric['label'] = km_metric.index\n",
    "km_metric "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "pd.plotting.parallel_coordinates(km_metric, class_column = 'label', color=sns.color_palette())\n",
    "plt.xticks(fontsize=10, rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pva_non_metric_km_labeled = pd.concat((ohc_pva, pd.Series(km_labels, name='labels', index=pva_standard.index.tolist())), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_state = pva_non_metric_km_labeled.groupby('labels').mean().iloc[:,:57]\n",
    "km_state['label'] = km_state.index\n",
    "km_state "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "pd.plotting.parallel_coordinates(km_state, class_column = 'label', color=sns.color_palette())\n",
    "plt.xticks(fontsize=10, rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "km_gender = pva_non_metric_km_labeled.groupby('labels').mean().iloc[:,57:61]\n",
    "km_gender['label'] = km_gender.index\n",
    "km_gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "pd.plotting.parallel_coordinates(km_gender, class_column = 'label', color=sns.color_palette())\n",
    "plt.xticks(fontsize=10, rotation=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Urbanicity Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "km_urbancity = pva_non_metric_km_labeled.groupby('labels').mean().iloc[:,61:66]\n",
    "km_urbancity['label'] = km_urbancity.index\n",
    "km_urbancity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "pd.plotting.parallel_coordinates(km_urbancity, class_column = 'label', color=sns.color_palette())\n",
    "plt.xticks(fontsize=10, rotation=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_interests = pva_non_metric_km_labeled.groupby('labels').mean().iloc[:,66:]\n",
    "km_interests['label'] = km_interests.index\n",
    "km_interests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "pd.plotting.parallel_coordinates(km_interests, class_column = 'label', color=sns.color_palette())\n",
    "plt.xticks(fontsize=10, rotation=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_dim_km = TSNE(random_state=42).fit_transform(pva_standard_km_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(two_dim_km).plot.scatter(x=0, y=1, c = pva_standard_km_labeled['labels'], colormap='viridis', figsize=(15,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical and K-means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define large number of K-means clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters = 30, init = 'k-means++', n_init = 10, random_state = 0)\n",
    "km.fit(pva_standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_labels_2 = km.predict(pva_standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_labeled_2 = pd.concat((pva_standard, pd.Series(km_labels_2, name='Labels_km', index=pva_standard.index.tolist())), axis=1)\n",
    "km_centroids_2 = km_labeled_2.groupby('Labels_km').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_labeled_metric_2 = pd.concat((pva_metric, pd.Series(km_labels_2, name='Labels_km', index=pva_metric.index.tolist())), axis=1)\n",
    "km_centroids_metric_2 = km_labeled_metric_2.groupby('Labels_km').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_labeled_non_metric_2 = pd.concat((ohc_pva, pd.Series(km_labels_2, name='Labels_km', index=pva_standard.index.tolist())), axis=1)\n",
    "km_centriods_non_metric_2  = km_labeled_non_metric_2.groupby('Labels_km').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining number of hierarquical clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting distance_threshold=0 and n_clusters=None ensures we compute the full tree\n",
    "linkage = 'ward'\n",
    "distance = 'euclidean'\n",
    "hclust_1 = AgglomerativeClustering(linkage=linkage, affinity=distance, n_clusters=None, distance_threshold=0.000001)\n",
    "hclust_1.fit_predict(km_centroids_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from:\n",
    "# create the counts of samples under each node (number of points being merged)\n",
    "counts = np.zeros(hclust_1.children_.shape[0])\n",
    "n_samples = len(hclust_1.labels_)\n",
    "\n",
    "# hclust.children_ contains the observation ids that are being merged together\n",
    "# At the i-th iteration, children[i][0] and children[i][1] are merged to form node n_samples + i\n",
    "for i, merge in enumerate(hclust_1.children_):\n",
    "    # track the number of observations in the current cluster being formed\n",
    "    current_count = 0\n",
    "    for child_idx in merge:\n",
    "        if child_idx < n_samples:\n",
    "            # If this is True, then we are merging an observation\n",
    "            current_count += 1  # leaf node\n",
    "        else:\n",
    "            # Otherwise, we are merging a previously formed cluster\n",
    "            current_count += counts[child_idx - n_samples]\n",
    "    counts[i] = current_count\n",
    "\n",
    "# the hclust.children_ is used to indicate the two points/clusters being merged (dendrogram's u-joins)\n",
    "# the hclust.distances_ indicates the distance between the two points/clusters (height of the u-joins)\n",
    "# the counts indicate the number of points being merged (dendrogram's x-axis)\n",
    "linkage_matrix = np.column_stack(\n",
    "    [hclust_1.children_, hclust_1.distances_, counts]\n",
    ").astype(float)\n",
    "\n",
    "# Plot the corresponding dendrogram\n",
    "sns.set()\n",
    "fig = plt.figure(figsize=(11,5))\n",
    "# The Dendrogram parameters need to be tuned\n",
    "y_threshold = 6\n",
    "dendrogram(linkage_matrix, truncate_mode='level', p=5, color_threshold=y_threshold, above_threshold_color='k')\n",
    "plt.hlines(y_threshold, 0, 1000, colors=\"r\", linestyles=\"dashed\")\n",
    "plt.title(f'Hierarchical Clustering - {linkage.title()}\\'s Dendrogram', fontsize=21)\n",
    "plt.xlabel('Number of points in node (or index of point if no parenthesis)')\n",
    "plt.ylabel(f'{distance.title()} Distance', fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical on top of K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Performing Hierarchical Clustering\n",
    "hc_km_cluster = AgglomerativeClustering(linkage='ward', affinity = 'euclidean', n_clusters = 5)\n",
    "hc_km_labels = hc_km_cluster.fit_predict(km_centroids_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Characterizing Hierarchical on top of K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hc_km_labeled_standard = pd.concat((km_centroids_2, pd.Series(hc_km_labels, name='Labels_hc')), axis=1)\n",
    "hc_km_standard = hc_km_labeled_standard.groupby('Labels_hc').mean()\n",
    "hc_km_standard['Label_hc'] = hc_km_standard.index\n",
    "hc_km_standard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "pd.plotting.parallel_coordinates(hc_km_standard, class_column = 'Label_hc', color=sns.color_palette())\n",
    "plt.xticks(fontsize=10, rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hc_km_labeled_metric = pd.concat((km_centroids_metric_2, pd.Series(hc_km_labels, name='Labels_hc')), axis=1)\n",
    "hc_km_metric = hc_km_labeled_metric.groupby('Labels_hc').mean()\n",
    "hc_km_metric['Label_hc'] = hc_km_metric.index\n",
    "hc_km_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "pd.plotting.parallel_coordinates(hc_km_metric, class_column = 'Label_hc', color=sns.color_palette())\n",
    "plt.xticks(fontsize=10, rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc_km_labeled_non_metric = pd.concat((km_centriods_non_metric_2, pd.Series(hc_km_labels, name='Labels_hc')), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc_km_state = hc_km_labeled_non_metric.groupby('Labels_hc', as_index=False).mean().iloc[:,1:57]\n",
    "hc_km_state['label'] = hc_km_state.index\n",
    "hc_km_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "pd.plotting.parallel_coordinates(hc_km_state, class_column = 'label', color=sns.color_palette())\n",
    "plt.xticks(fontsize=10, rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hc_km_gender = hc_km_labeled_non_metric.groupby('Labels_hc', as_index=False).mean().iloc[:,57:61]\n",
    "hc_km_gender['label'] = hc_km_gender.index\n",
    "hc_km_gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(hc_km_labeled_non_metric.iloc[:,61:66])\n",
    "plt.figure(figsize=(15, 7))\n",
    "pd.plotting.parallel_coordinates(hc_km_gender, class_column = 'label', color=sns.color_palette())\n",
    "plt.xticks(fontsize=10, rotation=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Urbanicity Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hc_km_urbanicity = hc_km_labeled_non_metric.groupby('Labels_hc', as_index=False).mean().iloc[:,61:66]\n",
    "hc_km_urbanicity['label'] = hc_km_urbanicity.index\n",
    "hc_km_urbanicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(hc_km_labeled_non_metric.iloc[:,61:66])\n",
    "plt.figure(figsize=(15, 7))\n",
    "pd.plotting.parallel_coordinates(hc_km_urbanicity, class_column = 'label', color=sns.color_palette())\n",
    "plt.xticks(fontsize=10, rotation=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hc_km_interests = hc_km_labeled_non_metric.groupby('Labels_hc', as_index=False).mean().iloc[:,66:]\n",
    "hc_km_interests['label'] = hc_km_interests.index\n",
    "hc_km_interests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "pd.plotting.parallel_coordinates(hc_km_interests, class_column = 'label', color=sns.color_palette())\n",
    "plt.xticks(fontsize=10, rotation=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization K-means on top of Hierarchical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_hc_tsne = km_labeled_2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_hc_tsne['Labels_km'].replace({0: 4, 1: 2, 2: 0, 3: 1, 4: 2, 5: 2, 6: 0, 7: 0, 8: 0, 9: 2, 10: 0, 11: 3, 12: 2, 13: 1, 14: 0, 15: 0, 16: 3, 17: 0, 18: 1, 19: 0, 20: 3, 21: 1, 22: 0, 23: 3, 24: 4, 25: 2, 26: 2, 27: 0, 28: 1, 29: 4}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_hc_tsne.rename(columns={'Labels_km':'Labels_km_hc'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_dim_km_hc = TSNE(random_state=42).fit_transform(km_hc_tsne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(two_dim_km_hc).plot.scatter(x=0, y=1, c = km_hc_tsne['Labels_km_hc'], colormap='viridis', figsize=(15,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering by Perspectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means Clustering by Perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Demographic = pva_standard[['INCOME', 'LOG_AGE', 'LOG_AVGGIFT', 'LASTGIFT', 'MINRAMNT', 'LOG_RAMNTALL']]\n",
    "Engagement =  pva_standard[['YEARS_FIRST_GIFT', 'NGIFTALL', 'GIFT_FREQUENCY', 'TIMELAG', 'HIT', 'CARDGIFT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Demographic_metric = pva_metric[['INCOME', 'LOG_AGE', 'LOG_AVGGIFT', 'LASTGIFT', 'MINRAMNT', 'LOG_RAMNTALL']]\n",
    "Engagement_metric =  pva_metric[['YEARS_FIRST_GIFT', 'NGIFTALL', 'GIFT_FREQUENCY', 'TIMELAG', 'HIT', 'CARDGIFT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ss(df):\n",
    "    \"\"\"Computes the sum of squares for all variables given a dataset\n",
    "    \"\"\"\n",
    "    ss = np.sum(df.var() * (df.count() - 1))\n",
    "    return ss  # return sum of sum of squares of each df variable\n",
    "\n",
    "def r2(df, labels):\n",
    "    sst = get_ss(df)\n",
    "    ssw = np.sum(df.groupby(labels).apply(get_ss))\n",
    "    return 1 - ssw/sst\n",
    "    \n",
    "def get_r2_scores(df, clusterer, min_k=2, max_k=10):\n",
    "    \"\"\"\n",
    "    Loop over different values of k. To be used with sklearn clusterers.\n",
    "    \"\"\"\n",
    "    r2_clust = {}\n",
    "    for n in range(min_k, max_k):\n",
    "        clust = clone(clusterer).set_params(n_clusters=n)\n",
    "        labels = clust.fit_predict(df)\n",
    "        r2_clust[n] = r2(df, labels)\n",
    "    return r2_clust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set up the clusterers\n",
    "kmeans = KMeans(init='k-means++', n_init=15, random_state=42)\n",
    "\n",
    "\n",
    "# Obtaining the R² scores for each cluster solution on product variables\n",
    "r2_scores = {}\n",
    "r2_scores['kmeans_Demo'] = get_r2_scores(Demographic, kmeans)\n",
    "r2_scores['kmeans_Eng'] = get_r2_scores(Engagement, kmeans)\n",
    "\n",
    "\n",
    "# Visualizing the R² scores for each cluster solution on product variables\n",
    "pd.DataFrame(r2_scores).plot.line(figsize=(10,7))\n",
    "\n",
    " \n",
    "plt.title(\"Product Variables:\\nR2 plot for various clustering methods\\n\", fontsize=21)\n",
    "plt.legend(title = 'Cluster methods', title_fontsize=11)\n",
    "plt.xlabel('Number of clusters', fontsize=13)\n",
    "plt.ylabel('R2 metric', fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_Demo = KMeans(n_clusters = 6, init = 'k-means++', n_init = 15, random_state=42)\n",
    "Demo_labels = kmeans_Demo.fit_predict(Demographic)\n",
    "\n",
    "kmeans_Eng = KMeans(n_clusters = 7, init = 'k-means++', n_init = 15, random_state = 42)\n",
    "Eng_labels = kmeans_Eng.fit_predict(Engagement)\n",
    "\n",
    "km_metric_perspective = pva_standard.copy()\n",
    "\n",
    "km_metric_perspective['Demo_labels'] = Demo_labels\n",
    "km_metric_perspective['Eng_labels'] = Eng_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "km_metric_perspective.groupby(['Demo_labels', 'Eng_labels'])\\\n",
    "    .size()\\\n",
    "    .to_frame()\\\n",
    "    .reset_index()\\\n",
    "    .pivot('Demo_labels', 'Eng_labels', 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging Using Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perspective_centroids = km_metric_perspective.groupby(['Demo_labels', 'Eng_labels']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkage = 'ward'\n",
    "hclust = AgglomerativeClustering(linkage = linkage, affinity='euclidean', distance_threshold = 0, n_clusters= None) # distance_threshold = 0\n",
    "hclust_labels = hclust.fit_predict(perspective_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from:\n",
    "# https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html#sphx-glr-auto-examples-cluster-plot-agglomerative-dendrogram-py\n",
    "\n",
    "# create the counts of samples under each node (number of points being merged)\n",
    "counts = np.zeros(hclust.children_.shape[0])\n",
    "n_samples = len(hclust.labels_)\n",
    "\n",
    "# hclust.children_ contains the observation ids that are being merged together\n",
    "# At the i-th iteration, children[i][0] and children[i][1] are merged to form node n_samples + i\n",
    "for i, merge in enumerate(hclust.children_):\n",
    "    # track the number of observations in the current cluster being formed\n",
    "    current_count = 0\n",
    "    for child_idx in merge:\n",
    "        if child_idx < n_samples:\n",
    "            # If this is True, then we are merging an observation\n",
    "            current_count += 1  # leaf node\n",
    "        else:\n",
    "            # Otherwise, we are merging a previously formed cluster\n",
    "            current_count += counts[child_idx - n_samples]\n",
    "    counts[i] = current_count\n",
    "\n",
    "# the hclust.children_ is used to indicate the two points/clusters being merged (dendrogram's u-joins)\n",
    "# the hclust.distances_ indicates the distance between the two points/clusters (height of the u-joins)\n",
    "# the counts indicate the number of points being merged (dendrogram's x-axis)\n",
    "linkage_matrix = np.column_stack(\n",
    "    [hclust.children_, hclust.distances_, counts]\n",
    ").astype(float)\n",
    "\n",
    " \n",
    "\n",
    "# Plot the corresponding dendrogram\n",
    "sns.set()\n",
    "fig = plt.figure(figsize=(11,5))\n",
    "# The Dendrogram parameters need to be tuned\n",
    "y_threshold=6\n",
    "dendrogram(linkage_matrix, truncate_mode='level', labels = perspective_centroids.index, p=5, above_threshold_color='k')\n",
    "plt.hlines(y_threshold, 0, 1000, colors=\"r\", linestyles=\"dashed\")\n",
    "plt.title(f'Hierarchical Clustering - {linkage.title()}\\'s Dendrogram', fontsize=21)\n",
    "plt.xlabel('Number of points in node (or index of point if no parenthesis)')\n",
    "plt.ylabel(f'Euclidean Distance', fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Re-running the Hierarchical clustering based on the correct number of clusters\n",
    "hclust = AgglomerativeClustering(linkage='ward', affinity='euclidean', n_clusters = 5)\n",
    "hclust_labels = hclust.fit_predict(perspective_centroids)\n",
    "perspective_centroids['hclust_labels'] = hclust_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Mapper between concatenated clusters and hierarchical clusters\n",
    "cluster_mapper = perspective_centroids['hclust_labels'].to_dict()\n",
    "\n",
    "#perspective_centroids_2 = perspective_centroids.copy()\n",
    "\n",
    "# Mapping the hierarchical clusters on the centroids to the observations\n",
    "km_metric_perspective['merged_labels'] = km_metric_perspective.apply(lambda row: cluster_mapper[(row['Demo_labels'], row['Eng_labels'])], axis=1)\n",
    "\n",
    "# Merged cluster centroids\n",
    "km_metric_perspective.groupby('merged_labels').mean()#[metric_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Characterizing Clusters by Perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_hc_metric_perspective = pva_metric.copy()\n",
    "\n",
    "km_hc_metric_perspective['Demo_labels'], km_hc_metric_perspective['Eng_labels'], km_hc_metric_perspective['merged_labels'] = km_metric_perspective['Demo_labels'],km_metric_perspective['Eng_labels'], km_metric_perspective['merged_labels'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_profiles(df, label_columns, figsize, compar_titles=None):\n",
    "    \"\"\"\n",
    "    Pass df with labels columns of one or multiple clustering labels. \n",
    "    Then specify this label columns to perform the cluster profile according to them.\n",
    "    \"\"\"\n",
    "    if compar_titles == None:\n",
    "        compar_titles = [\"\"]*len(label_columns)\n",
    "        \n",
    "    sns.set()\n",
    "    fig, axes = plt.subplots(nrows=len(label_columns), ncols=2, figsize=figsize, squeeze=False)\n",
    "    for ax, label, titl in zip(axes, label_columns, compar_titles):\n",
    "        # Filtering df\n",
    "        drop_cols = [i for i in label_columns if i!=label]\n",
    "        dfax = df.drop(drop_cols, axis=1)\n",
    "        \n",
    "        # Getting the cluster centroids and counts\n",
    "        centroids = dfax.groupby(by=label, as_index=False).mean()\n",
    "        counts = dfax.groupby(by=label, as_index=False).count().iloc[:,[0,1]]\n",
    "        counts.columns = [label, \"counts\"]\n",
    "        \n",
    "        # Setting Data\n",
    "        pd.plotting.parallel_coordinates(centroids, label, color=sns.color_palette(), ax=ax[0])\n",
    "        sns.barplot(x=label, y=\"counts\", data=counts, ax=ax[1])\n",
    "\n",
    " \n",
    "\n",
    "        #Setting Layout\n",
    "        handles, _ = ax[0].get_legend_handles_labels()\n",
    "        cluster_labels = [\"Cluster {}\".format(i) for i in range(len(handles))]\n",
    "        ax[0].annotate(text=titl, xy=(0.95,1.1), xycoords='axes fraction', fontsize=13, fontweight = 'heavy') \n",
    "        ax[0].legend(handles, cluster_labels) # Adaptable to number of clusters\n",
    "        ax[0].axhline(color=\"black\", linestyle=\"--\")\n",
    "        ax[0].set_title(\"Cluster Means - {} Clusters\".format(len(handles)), fontsize=13)\n",
    "        ax[0].set_xticklabels(ax[0].get_xticklabels(), rotation=-20)\n",
    "        ax[1].set_xticklabels(cluster_labels)\n",
    "        ax[1].set_xlabel(\"\")\n",
    "        ax[1].set_ylabel(\"Absolute Frequency\")\n",
    "        ax[1].set_title(\"Cluster Sizes - {} Clusters\".format(len(handles)), fontsize=13)\n",
    "    \n",
    "    plt.subplots_adjust(hspace=0.4, top=0.90)\n",
    "    plt.suptitle(\"Cluster Simple Profilling\", fontsize=23)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profilling each cluster (product, behavior, merged)\n",
    "cluster_profiles(\n",
    "    km_metric_perspective, \n",
    "    label_columns = ['Demo_labels', 'Eng_labels', 'merged_labels'],\n",
    "    figsize = (28, 13),\n",
    "    compar_titles = [\"Demographic Variables Clustering\", \"Engagement Variables Clustering\", \"Merged Clustering\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profilling each cluster (product, behavior, merged)\n",
    "cluster_profiles(\n",
    "    km_hc_metric_perspective, \n",
    "    label_columns = ['Demo_labels', 'Eng_labels', 'merged_labels'],\n",
    "    figsize = (28, 13),\n",
    "    compar_titles = [\"Demographic Variables Clustering\", \"Engagement Variables Clustering\", \"Merged Clustering\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Visualization using TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is step can be quite time consuming\n",
    "two_dim = TSNE(random_state=42).fit_transform(km_metric_perspective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # t-SNE visualization\n",
    "pd.DataFrame(two_dim).plot.scatter(x=0, y=1, c = km_metric_perspective['Demo_labels'], colormap='viridis', figsize=(15,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # t-SNE visualization\n",
    "pd.DataFrame(two_dim).plot.scatter(x=0, y=1, c = km_metric_perspective['Eng_labels'], colormap='viridis', figsize=(15,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(two_dim).plot.scatter(x=0, y=1, c = km_metric_perspective['merged_labels'], colormap='viridis', figsize=(15,10))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
